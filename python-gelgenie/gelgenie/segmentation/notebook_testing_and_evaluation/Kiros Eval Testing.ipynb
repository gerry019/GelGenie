{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0088d2ca",
   "metadata": {},
   "source": [
    "import segmentation.unet\n",
    "import importlib\n",
    "importlib.reload(segmentation.unet)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2467b761",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "import imageio\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from segmentation.unet import UNet, smp_UNetPlusPlus, smp_UNet\n",
    "from segmentation.helper_functions.display_functions import show_segmentation"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8dffdc4",
   "metadata": {},
   "source": [
    "# code here modified from https://github.com/milesial/Pytorch-UNet/tree/e36c782fbfc976b7326182a47dd7213bd3360a7e\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from segmentation.helper_functions.general_functions import extract_image_names_from_folder\n",
    "\n",
    "\n",
    "class BasicDataset(Dataset):\n",
    "    def __init__(self, images_dir: str, n_channels: int, scale: float = 1.0,\n",
    "                 augmentations=None, padding: bool = False, image_names=None):\n",
    "        \"\"\"\n",
    "        TODO: fill in documentation\n",
    "        :param images_dir:\n",
    "        :param masks_dir:\n",
    "        :param n_channels:\n",
    "        :param scale:\n",
    "        :param mask_suffix:\n",
    "        :param augmentations:\n",
    "        :param padding:\n",
    "        \"\"\"\n",
    "\n",
    "        assert (n_channels == 1 or n_channels == 3), 'Dataset number of channels must be either 1 or 3'\n",
    "        assert 0 < scale <= 1, 'Image scaling must be between 0 and 1'\n",
    "\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.n_channels = n_channels\n",
    "        self.scale = scale\n",
    "        self.standard_image_transform = transforms.Compose([transforms.ToTensor()])\n",
    "        if image_names is not None:\n",
    "            self.image_names = image_names\n",
    "        else:\n",
    "            self.image_names = extract_image_names_from_folder(images_dir)\n",
    "\n",
    "        self.augmentations = augmentations\n",
    "        self.padding = padding\n",
    "\n",
    "        if padding:\n",
    "            max_dimension = 0\n",
    "            # loops through provided images and extracts the largest image dimension, for use if padding is selected\n",
    "            for root, dirs, files in os.walk(self.images_dir):\n",
    "                for name in files:\n",
    "                    image_file = os.path.join(root, name)\n",
    "                    image = imageio.imread(image_file)  # TODO: investigate the warning here...\n",
    "                    max_dimension = max(max_dimension, image.shape[0], image.shape[1])\n",
    "            max_dimension = 32 * (max_dimension // 32 + 1)  # to be divisible by 32 TODO: why?\n",
    "\n",
    "            self.max_dimension = max_dimension\n",
    "\n",
    "        if not self.image_names:\n",
    "            raise RuntimeError(f'No images found in {images_dir}, make sure you put your images there')\n",
    "        logging.info(f'Creating dataset with {len(self.image_names)} examples')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def set_augmentations(self, augmentations):\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    @staticmethod\n",
    "    def load_image(self, filename, n_channels):\n",
    "        image = imageio.imread(filename)\n",
    "\n",
    "        # Converts to desired number of channels\n",
    "        if n_channels == 1:  # Target input: 1 channel\n",
    "            if image.shape[-1] == 3:  # Actual input: 3 channels\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "            elif image.shape[-1] == 4:  # Actual input: 4 channels\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGBA2GRAY)\n",
    "            # No change required for already grayscale images\n",
    "        elif n_channels == 3:  # Target input: 3 channels\n",
    "            if image.shape[-1] == 4:  # Actual input: 4 channels\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "            elif image.shape[-1] != 3:  # Actual input: 1 channels\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        # Normalizing image\n",
    "        if image.dtype == 'uint8':\n",
    "            max_val = 255\n",
    "        elif image.dtype == 'uint16':\n",
    "            max_val = 65535\n",
    "        else:\n",
    "            raise RuntimeError('Image type not recognized.')\n",
    "\n",
    "        image = image.astype(np.float32) / max_val\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print('Dataloader is %s, image IDX is: %s, image_name is %s' % ('validation' if not self.augmentations else 'Training', idx, self.image_names[idx]))\n",
    "        # return np.zeros((5,5))\n",
    "        img_file = self.image_names[idx]\n",
    "        \n",
    "        img_array = self.load_image(self, filename=img_file, n_channels=self.n_channels)\n",
    "\n",
    "        if self.augmentations:\n",
    "            sample = self.augmentations(image=img_array, mask=mask_array)\n",
    "            img_array = sample['image']\n",
    "            mask_array = sample['mask']\n",
    "\n",
    "        if self.padding:\n",
    "            top = (self.max_dimension - img_array.shape[0]) // 2\n",
    "            bottom = self.max_dimension - img_array.shape[0] - top\n",
    "            left = (self.max_dimension - img_array.shape[1]) // 2\n",
    "            right = self.max_dimension - img_array.shape[1] - left\n",
    "\n",
    "            img_array = np.pad(img_array, pad_width=((top, bottom), (left, right)), mode='constant')\n",
    "\n",
    "        img_tensor = self.standard_image_transform(img_array)\n",
    "        \n",
    "        return img_tensor\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f41a6f7",
   "metadata": {},
   "source": [
    "def test_net(images_path, n_channels, checkpoint_file_path, net):\n",
    "    test_set = BasicDataset(images_path, n_channels, padding=True)\n",
    "    n_test = int(len(test_set))\n",
    "    test_loader = DataLoader(test_set, shuffle=False, batch_size=1, num_workers=0, pin_memory=True)\n",
    "    net.train()\n",
    "    saved_dict = torch.load(f=checkpoint_file_path, map_location=torch.device(\"cpu\"))\n",
    "    net.load_state_dict(saved_dict['network'])\n",
    "    # net.load_state_dict(saved_dict)\n",
    "    print(f'Model loaded from {checkpoint_file_path}')\n",
    "\n",
    "    fig, ax = plt.subplots(n_test, 3, figsize=(10, 120))\n",
    "\n",
    "    plot_row = 0\n",
    "    for image in test_loader:\n",
    "        with torch.no_grad():\n",
    "            mask_pred = net(image)\n",
    "        image = image.squeeze()\n",
    "        mask_pred.squeeze()\n",
    "\n",
    "\n",
    "        mask_pred_array = np.transpose(mask_pred.detach().squeeze().cpu().numpy(), (1, 2, 0))  # CHW to HWC\n",
    "        height, width = mask_pred_array.shape[0], mask_pred_array.shape[1]\n",
    "\n",
    "        threshold = 0.8\n",
    "        thresholded = np.zeros((height, width))\n",
    "        for row in range(height):\n",
    "            for column in range(width):\n",
    "                if mask_pred_array[row][column][0] < (1-threshold) and mask_pred_array[row][column][1] > threshold:\n",
    "                    thresholded[row][column] = 1\n",
    "\n",
    "\n",
    "\n",
    "        # use a boolean condition to find where pixel values are > 0.75\n",
    "        blobs = thresholded == 1\n",
    "\n",
    "        # label connected regions that satisfy this condition\n",
    "        labels, nlabels = ndimage.label(blobs, structure=[[1,1,1],[1,1,1],[1,1,1]])\n",
    "\n",
    "\n",
    "        # find their centres of mass. in this case I'm weighting by the pixel values in\n",
    "        # `img`, but you could also pass the boolean values in `blobs` to compute the\n",
    "        # unweighted centroids.\n",
    "        r, c = np.vstack(ndimage.center_of_mass(thresholded, labels, np.arange(nlabels) + 1)).T\n",
    "\n",
    "        # find their distances from the top-left corner\n",
    "        d = np.sqrt(r*r + c*c)\n",
    "\n",
    "\n",
    "        # Get coordinates for each unique band\n",
    "        # create array of intensities\n",
    "        volume_labels = np.zeros((nlabels+1), float)\n",
    "        area_labels = np.zeros((nlabels+1), int)\n",
    "\n",
    "        for h in range(height):\n",
    "            for w in range(width):\n",
    "                volume_labels[labels[h][w]] += image[h][w]  # index = label, value += intensity(between 0 and 1)\n",
    "                area_labels[labels[h][w]] += 1\n",
    "        # print(f'(nlabels = {nlabels})\\nvolume_labels: {volume_labels}')\n",
    "\n",
    "    # plot\n",
    "\n",
    "    # ax[0].imshow(thresholded)\n",
    "        original_image = image.detach().squeeze().cpu().numpy()\n",
    "        ax[plot_row][0].imshow(original_image, cmap='gray')\n",
    "\n",
    "        ax[plot_row][1].imshow(original_image, cmap='gray')\n",
    "        ax[plot_row][1].imshow(np.ma.masked_array(labels, ~blobs), cmap=plt.cm.rainbow)\n",
    "#         for ri, ci, di in zip(r, c, d):\n",
    "#             label = int(labels[int(ri)][int(ci)])\n",
    "#             ax[plot_row][1].annotate(f'{label}', xy=(ci, ri),  xytext=(0, -5),\n",
    "#                        textcoords='offset points', ha='center', va='top',\n",
    "#                        fontsize=8, color='blue')\n",
    "\n",
    "        ax[plot_row][2].imshow(np.ma.masked_array(labels, ~blobs), cmap=plt.cm.rainbow)\n",
    "        for ri, ci, di, count in zip(r, c, d, range(nlabels)):\n",
    "        #     ax[1].annotate('', xy=(0, 0), xytext=(ci, ri),\n",
    "        #                    arrowprops={'arrowstyle':'<-', 'shrinkA':0})\n",
    "        #     ax[1].annotate(f'label={label}, concentration={volume_labels[label]}', xy=(ci, ri),  xytext=(0, -5),\n",
    "        #     textcoords='offset points', ha='center', va='top',\n",
    "            label = int(labels[int(ri)][int(ci)])\n",
    "            ax[plot_row][2].annotate(f'{count+1}: V={round(volume_labels[count+1], 1)}', xy=(ci, ri),  xytext=(0, -5),\n",
    "                                  textcoords='offset points', ha='center', va='top',\n",
    "                                  fontsize=8)\n",
    "        plot_row+=1\n",
    "        \n",
    "        # TODO: delete\n",
    "        print(volume_labels)\n",
    "        \n",
    "        \n",
    "        \n",
    "        break\n",
    "    for aa in ax.flat:\n",
    "        aa.set_axis_off()\n",
    "    fig.tight_layout()\n",
    "    plt.savefig('C:/Users/s2137314/Downloads/save as pdf.pdf')\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8beae702",
   "metadata": {},
   "source": [
    "images_path = \"C:/2022_Summer_Intern/NEB_Gel_Images\"\n",
    "# checkpoint_file_path = \"C:/Users/s2137314/Downloads/checkpoint_epoch450.pth\"\n",
    "# checkpoint_file_path = \"C:/Users/s2137314/Downloads/smp-UNet-with-augmentations-checkpoint_epoch300.pth\"\n",
    "# checkpoint_file_path = \"C:/2022_Summer_Intern/smp-UNet_No_Augmentations_epoch_180/smp-UNet-no-augmentations-checkpoint_epoch180.pth\"\n",
    "checkpoint_file_path = \"C:/2022_Summer_Intern/smp-UNet_No_pretrain_epoch_260_0.8553/smp-UNet_pretrain_imagenet_checkpoint_epoch260.pth\"\n",
    "n_channels = 1\n",
    "# net = UNet(n_channels=1, n_classes=2, bilinear=False)\n",
    "net = smp_UNet(\n",
    "            encoder_name=\"resnet18\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "            in_channels=1,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "            classes=2,  # model output channels (number of classes in your dataset)\n",
    "        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ffa4354",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "test_net(images_path, n_channels, checkpoint_file_path, net)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ee39b85",
   "metadata": {},
   "source": [
    "real_volumes = []\n",
    "analyzer_volumes = []\n",
    "filter_indices = [1, 2, 3, 4, 6, 7, 8, 9, 12, 14]\n",
    "measured_volumes = volume_labels[filter_indices]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "497cf409",
   "metadata": {},
   "source": [
    "1 2 3 4 5 7 8 9 12 14"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f732d236",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "measured_ratios = []\n",
    "prev_measured_ratios = []\n",
    "real_ratios = []\n",
    "analyzer_ratios = []\n",
    "prev_measured_volumes = [154.4, 176.5, 249.7, 166.6, 187.7, 477.4, 152.1, 155.2, 193.8, 145.7]\n",
    "measured_volumes = [151.2, 181.4, 227.2, 209.4, 179.8, 503.7, 225.7,179.9, 165.8, 216.0]\n",
    "real_volumes = [42, 42, 50, 42, 33, 125, 48, 36, 42, 42]\n",
    "# measured_volumes = volume_labels[1:]\n",
    "analyzer_volumes = [1320, 1423, 1854, 1759, 1667, 3299, 1552, 1341, 1278, 1465]\n",
    "base_value_real = 33\n",
    "prev_base_value_measured = 154.4\n",
    "base_value_measured = 151.2\n",
    "base_value_analyzer = 1320\n",
    "\n",
    "for r, pm, m, a in zip(real_volumes, prev_measured_volumes, measured_volumes, analyzer_volumes):\n",
    "    real_ratios.append(r/base_value_real)\n",
    "    prev_measured_ratios.append(pm/prev_base_value_measured)\n",
    "    measured_ratios.append(m/base_value_measured)\n",
    "    analyzer_ratios.append(a/base_value_analyzer)\n",
    "print(\"Real // Measured // GelAnalyzer\")\n",
    "print(np.c_[real_ratios, measured_ratios, analyzer_ratios])\n",
    "x = range(1,11)\n",
    "plt.plot(x, real_ratios, color='black', label='real ratios')\n",
    "plt.plot(x, measured_ratios, color='red', label='measured ratios')\n",
    "plt.plot(x, prev_measured_ratios, color='blue', label='previous measured ratios')\n",
    "plt.plot(x, analyzer_ratios, color='grey', label='gel-analyzer ratios')\n",
    "plt.legend()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b39d11d",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "volume_labels[6]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "756713ff",
   "metadata": {},
   "source": [
    "volume_labels[7]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a3ab6bd",
   "metadata": {},
   "source": [
    "volume_labels[10]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64d94b04",
   "metadata": {},
   "source": [
    "nlabels"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a526c7",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
