{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-17T21:15:41.135831Z",
     "start_time": "2023-05-17T21:15:31.889931Z"
    }
   },
   "source": [
    "import segmentation.unet\n",
    "\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "import imageio\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from segmentation.unet import smp_UNet\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# code here modified from https://github.com/milesial/Pytorch-UNet/tree/e36c782fbfc976b7326182a47dd7213bd3360a7e\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from segmentation.helper_functions.general_functions import extract_image_names_from_folder\n",
    "\n",
    "\n",
    "class BasicDataset(Dataset):\n",
    "    def __init__(self, images_dir: str, n_channels: int, scale: float = 1.0,\n",
    "                 augmentations=None, padding: bool = False, image_names=None):\n",
    "        \"\"\"\n",
    "        TODO: fill in documentation\n",
    "        :param images_dir:\n",
    "        :param masks_dir:\n",
    "        :param n_channels:\n",
    "        :param scale:\n",
    "        :param mask_suffix:\n",
    "        :param augmentations:\n",
    "        :param padding:\n",
    "        \"\"\"\n",
    "\n",
    "        assert (n_channels == 1 or n_channels == 3), 'Dataset number of channels must be either 1 or 3'\n",
    "        assert 0 < scale <= 1, 'Image scaling must be between 0 and 1'\n",
    "\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.n_channels = n_channels\n",
    "        self.scale = scale\n",
    "        self.standard_image_transform = transforms.Compose([transforms.ToTensor()])\n",
    "        if image_names is not None:\n",
    "            self.image_names = image_names\n",
    "        else:\n",
    "            self.image_names = extract_image_names_from_folder(images_dir)\n",
    "\n",
    "        self.augmentations = augmentations\n",
    "        self.padding = padding\n",
    "\n",
    "        if padding:\n",
    "            max_dimension = 0\n",
    "            # loops through provided images and extracts the largest image dimension, for use if padding is selected\n",
    "            for root, dirs, files in os.walk(self.images_dir):\n",
    "                for name in files:\n",
    "                    if name == '.DS_Store':\n",
    "                        continue\n",
    "                    image_file = os.path.join(root, name)\n",
    "                    image = imageio.imread(image_file)  # TODO: investigate the warning here...\n",
    "                    max_dimension = max(max_dimension, image.shape[0], image.shape[1])\n",
    "            max_dimension = 32 * (max_dimension // 32 + 1)  # to be divisible by 32 TODO: why?\n",
    "\n",
    "            self.max_dimension = max_dimension\n",
    "\n",
    "        if not self.image_names:\n",
    "            raise RuntimeError(f'No images found in {images_dir}, make sure you put your images there')\n",
    "        logging.info(f'Creating dataset with {len(self.image_names)} examples')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def set_augmentations(self, augmentations):\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    @staticmethod\n",
    "    def load_image(self, filename, n_channels):\n",
    "        image = imageio.imread(filename)\n",
    "\n",
    "        # Converts to desired number of channels\n",
    "        if n_channels == 1:  # Target input: 1 channel\n",
    "            if image.shape[-1] == 3:  # Actual input: 3 channels\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "            elif image.shape[-1] == 4:  # Actual input: 4 channels\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGBA2GRAY)\n",
    "            # No change required for already grayscale images\n",
    "        elif n_channels == 3:  # Target input: 3 channels\n",
    "            if image.shape[-1] == 4:  # Actual input: 4 channels\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "            elif image.shape[-1] != 3:  # Actual input: 1 channels\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        # Normalizing image\n",
    "        if image.dtype == 'uint8':\n",
    "            max_val = 255\n",
    "        elif image.dtype == 'uint16':\n",
    "            max_val = 65535\n",
    "        else:\n",
    "            raise RuntimeError('Image type not recognized.')\n",
    "\n",
    "        image = image.astype(np.float32) / max_val\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print('Dataloader is %s, image IDX is: %s, image_name is %s' % ('validation' if not self.augmentations else 'Training', idx, self.image_names[idx]))\n",
    "        # return np.zeros((5,5))\n",
    "        img_file = self.image_names[idx]\n",
    "\n",
    "        img_array = self.load_image(self, filename=img_file, n_channels=self.n_channels)\n",
    "\n",
    "        if self.augmentations:\n",
    "            sample = self.augmentations(image=img_array, mask=mask_array)\n",
    "            img_array = sample['image']\n",
    "            mask_array = sample['mask']\n",
    "\n",
    "        if self.padding:\n",
    "            top = (self.max_dimension - img_array.shape[0]) // 2\n",
    "            bottom = self.max_dimension - img_array.shape[0] - top\n",
    "            left = (self.max_dimension - img_array.shape[1]) // 2\n",
    "            right = self.max_dimension - img_array.shape[1] - left\n",
    "\n",
    "            img_array = np.pad(img_array, pad_width=((top, bottom), (left, right)), mode='constant')\n",
    "\n",
    "        img_tensor = self.standard_image_transform(img_array)\n",
    "\n",
    "        return img_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T10:05:24.984903Z",
     "start_time": "2023-05-18T10:05:24.953027Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "images_path = \"../../../scratch_data/q2\"\n",
    "\n",
    "checkpoint_file_path = \"/Users/matt/Documents/PhD/research_output/Automatic_Gel_Analyzer/segmentation_models/smp-UNet_No_Augmentations/checkpoints/checkpoint_epoch390.pth\"\n",
    "n_channels = 1\n",
    "net = smp_UNet(\n",
    "            encoder_name=\"resnet18\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "            in_channels=1,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "            classes=2,  # model output channels (number of classes in your dataset)\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T10:06:23.013535Z",
     "start_time": "2023-05-18T10:06:22.414486Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "\n",
    "test_set = BasicDataset(images_path, n_channels, padding=True)\n",
    "n_test = int(len(test_set))\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=1, num_workers=0, pin_memory=True)\n",
    "net.eval()\n",
    "saved_dict = torch.load(f=checkpoint_file_path, map_location=torch.device(\"cpu\"))\n",
    "net.load_state_dict(saved_dict['network'])\n",
    "# net.load_state_dict(saved_dict)\n",
    "print(f'Model loaded from {checkpoint_file_path}')\n",
    "\n",
    "# fig, ax = plt.subplots(n_test, 3, figsize=(10, 120))\n",
    "\n",
    "plot_row = 0\n",
    "for image in test_loader:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mask_pred = net(image)\n",
    "    image = image.squeeze()\n",
    "    mask_pred.squeeze()\n",
    "\n",
    "    mask_pred_array = np.transpose(mask_pred.detach().squeeze().cpu().numpy(), (1, 2, 0))  # CHW to HWC\n",
    "    height, width = mask_pred_array.shape[0], mask_pred_array.shape[1]\n",
    "\n",
    "    threshold = 0.8\n",
    "    thresholded = np.zeros((height, width))\n",
    "    for row in range(height):\n",
    "        for column in range(width):\n",
    "            if mask_pred_array[row][column][0] < (1-threshold) and mask_pred_array[row][column][1] > threshold:\n",
    "                thresholded[row][column] = 1\n",
    "\n",
    "    # use a boolean condition to find where pixel values are > 0.75\n",
    "    blobs = thresholded == 1\n",
    "\n",
    "    # label connected regions that satisfy this condition\n",
    "    labels, nlabels = ndimage.label(blobs, structure=[[1,1,1],[1,1,1],[1,1,1]])\n",
    "\n",
    "\n",
    "    # find their centres of mass. in this case I'm weighting by the pixel values in\n",
    "    # `img`, but you could also pass the boolean values in `blobs` to compute the\n",
    "    # unweighted centroids.\n",
    "    r, c = np.vstack(ndimage.center_of_mass(thresholded, labels, np.arange(nlabels) + 1)).T\n",
    "\n",
    "    # find their distances from the top-left corner\n",
    "    d = np.sqrt(r*r + c*c)\n",
    "\n",
    "\n",
    "    # Get coordinates for each unique band\n",
    "    # create array of intensities\n",
    "    volume_labels = np.zeros((nlabels+1), float)\n",
    "    area_labels = np.zeros((nlabels+1), int)\n",
    "\n",
    "    for h in range(height):\n",
    "        for w in range(width):\n",
    "            volume_labels[labels[h][w]] += image[h][w]  # index = label, value += intensity(between 0 and 1)\n",
    "            area_labels[labels[h][w]] += 1\n",
    "    # print(f'(nlabels = {nlabels})\\nvolume_labels: {volume_labels}')\n",
    "\n",
    "# plot\n",
    "\n",
    "# ax[0].imshow(thresholded)\n",
    "    original_image = image.detach().squeeze().cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(original_image, cmap='gray')\n",
    "    plt.imshow(np.ma.masked_array(labels, ~blobs), cmap=plt.cm.rainbow)\n",
    "\n",
    "\n",
    "#     ax[plot_row][0].imshow(original_image, cmap='gray')\n",
    "#\n",
    "#     ax[plot_row][1].imshow(original_image, cmap='gray')\n",
    "#     ax[plot_row][1].imshow(np.ma.masked_array(labels, ~blobs), cmap=plt.cm.rainbow)\n",
    "# #         for ri, ci, di in zip(r, c, d):\n",
    "# #             label = int(labels[int(ri)][int(ci)])\n",
    "# #             ax[plot_row][1].annotate(f'{label}', xy=(ci, ri),  xytext=(0, -5),\n",
    "# #                        textcoords='offset points', ha='center', va='top',\n",
    "# #                        fontsize=8, color='blue')\n",
    "#\n",
    "#     ax[plot_row][2].imshow(np.ma.masked_array(labels, ~blobs), cmap=plt.cm.rainbow)\n",
    "#     for ri, ci, di, count in zip(r, c, d, range(nlabels)):\n",
    "#     #     ax[1].annotate('', xy=(0, 0), xytext=(ci, ri),\n",
    "#     #                    arrowprops={'arrowstyle':'<-', 'shrinkA':0})\n",
    "#     #     ax[1].annotate(f'label={label}, concentration={volume_labels[label]}', xy=(ci, ri),  xytext=(0, -5),\n",
    "#     #     textcoords='offset points', ha='center', va='top',\n",
    "#         label = int(labels[int(ri)][int(ci)])\n",
    "#         ax[plot_row][2].annotate(f'{count+1}: V={round(volume_labels[count+1], 1)}', xy=(ci, ri),  xytext=(0, -5),\n",
    "#                               textcoords='offset points', ha='center', va='top',\n",
    "#                               fontsize=8)\n",
    "#     plot_row+=1\n",
    "#\n",
    "#     # TODO: delete\n",
    "#     # print(volume_labels)\n",
    "#     break\n",
    "# for aa in ax.flat:\n",
    "#     aa.set_axis_off()\n",
    "# fig.tight_layout()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T15:11:27.273255Z",
     "start_time": "2023-05-18T15:11:24.170016Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(original_image, cmap='gray')\n",
    "plt.imshow(np.ma.masked_array(labels, ~blobs), cmap=plt.cm.rainbow)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T15:44:12.849355Z",
     "start_time": "2023-05-18T15:44:12.078596Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "np.unique(labeled_bands)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T16:45:29.413645Z",
     "start_time": "2023-05-18T16:45:29.368552Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "one_hot = F.one_hot(mask_pred.argmax(dim=1), net.n_classes).permute(0, 3, 1, 2).float()\n",
    "onn = one_hot.numpy().squeeze()\n",
    "\n",
    "input_marker_array = onn.argmax(axis=0)\n",
    "\n",
    "input_marker_array = ndi.binary_fill_holes(input_marker_array)\n",
    "labeled_bands, _ = ndi.label(input_marker_array)\n",
    "plt.imshow(labeled_bands)\n",
    "# plt.imshow(labeled_bands)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T16:42:17.203507Z",
     "start_time": "2023-05-18T16:42:16.768961Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import skimage\n",
    "from skimage.filters import sobel\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.color import label2rgb\n",
    "\n",
    "\n",
    "option = 'unique_bands'\n",
    "one_hot = F.one_hot(mask_pred.argmax(dim=1), net.n_classes).permute(0, 3, 1, 2).float()\n",
    "onn = one_hot.numpy().squeeze()\n",
    "\n",
    "if option == 'unique_bands':\n",
    "    input_marker_array = ndi.binary_fill_holes(onn.argmax(axis=0))\n",
    "    input_marker_array, _ = ndi.label(input_marker_array)\n",
    "    input_marker_array[input_marker_array > 0] = input_marker_array[input_marker_array > 0] + 1\n",
    "else:\n",
    "    input_marker_array = onn.argmax(axis=0)*2\n",
    "\n",
    "input_marker_array[original_image < 0.3] = 1\n",
    "\n",
    "\n",
    "# get thresholded array, convert to a marker array, then add background markers, and leave 0s for unfilled areas\n",
    "\n",
    "\n",
    "    # Use Sobel filter on original image to find elevation map\n",
    "elevation_map = sobel(original_image)\n",
    "\n",
    "# Apply the watershed algorithm itself, using the elevation map and markers\n",
    "segmentation = skimage.segmentation.watershed(elevation_map, input_marker_array)\n",
    "original_seg = np.copy(segmentation)\n",
    "\n",
    "if option != 'unique_bands':\n",
    "    # Fill holes and relabel bands, giving each a unique label\n",
    "    segmentation = ndi.binary_fill_holes(segmentation - 1)\n",
    "    labeled_bands, _ = ndi.label(segmentation)\n",
    "else:\n",
    "    labeled_bands = segmentation - 1\n",
    "\n",
    "# Overlay labels on original image\n",
    "image_label_overlay = label2rgb(labeled_bands, image=original_image)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(image_label_overlay)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T16:51:41.476710Z",
     "start_time": "2023-05-18T16:51:39.306228Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "view_image = np.copy(original_image)\n",
    "\n",
    "view_image[view_image < 0.15] = 0\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(input_marker_array,cmap='gray')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T16:32:15.367352Z",
     "start_time": "2023-05-18T16:32:15.103614Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "onn [1,0:5,0:5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T16:25:22.086401Z",
     "start_time": "2023-05-18T16:25:22.065677Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 15))\n",
    "ax[0].imshow(original_image, cmap='gray')\n",
    "ax[1].imshow(mask_pred_array[...,0], cmap=plt.cm.rainbow)\n",
    "ax[2].imshow(mask_pred_array[...,1], cmap=plt.cm.rainbow)\n",
    "# plt.imshow(np.ma.masked_array(labels, ~blobs), cmap=plt.cm.rainbow)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T15:52:23.357205Z",
     "start_time": "2023-05-18T15:52:22.562463Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "threshold = 0.99\n",
    "thresholded = np.zeros((height, width))\n",
    "for row in range(height):\n",
    "    for column in range(width):\n",
    "        if mask_pred_array[row][column][0] < (1-threshold) and mask_pred_array[row][column][1] > threshold:\n",
    "            thresholded[row][column] = 1\n",
    "\n",
    "plt.imshow(original_image, cmap='gray')\n",
    "plt.imshow(onn[0,0,...], cmap='jet',alpha=0.5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T16:02:21.179048Z",
     "start_time": "2023-05-18T16:02:18.579662Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "one_hot = F.one_hot(mask_pred.argmax(dim=1), net.n_classes).permute(0, 3, 1, 2).float()\n",
    "onn = one_hot.numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T16:01:09.618474Z",
     "start_time": "2023-05-18T16:01:09.530025Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "tstmask = mask_pred[0,:,0:3,0:3]\n",
    "tstmask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T16:06:57.829322Z",
     "start_time": "2023-05-18T16:06:57.816662Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "mask_pred[0,:,0:3,0:3].argmax(dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T16:09:53.599401Z",
     "start_time": "2023-05-18T16:09:53.580017Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "rez = F.one_hot(mask_pred[0,:,0:3,0:3].argmax(dim=0),net.n_classes)\n",
    "rez"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T16:09:56.784496Z",
     "start_time": "2023-05-18T16:09:56.765210Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "torch.arange(0, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T16:04:07.275612Z",
     "start_time": "2023-05-18T16:04:07.218878Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "F.one_hot(torch.arange(0, 5) )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T16:04:10.712543Z",
     "start_time": "2023-05-18T16:04:10.668167Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from torchinfo import summary\n",
    "net2 = smp_UNet(\n",
    "            encoder_name=\"resnet18\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "            encoder_weights=\"ssl\",  # use `imagenet` pretreined weights for encoder initialization\n",
    "            in_channels=1,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "            classes=2,  # model output channels (number of classes in your dataset)\n",
    "        )\n",
    "images_path = \"../../../scratch_data/q2\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T10:01:33.390906Z",
     "start_time": "2023-05-18T10:01:33.095341Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "test_set = BasicDataset(images_path, n_channels, padding=True)\n",
    "n_test = int(len(test_set))\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=1, num_workers=0, pin_memory=True)\n",
    "net2.eval()\n",
    "# net.load_state_dict(saved_dict)\n",
    "# fig, ax = plt.subplots(n_test, 3, figsize=(10, 120))\n",
    "\n",
    "plot_row = 0\n",
    "for image in test_loader:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mask_pred = net2(image)\n",
    "    image = image.squeeze()\n",
    "    mask_pred.squeeze()\n",
    "\n",
    "    mask_pred_array = np.transpose(mask_pred.detach().squeeze().cpu().numpy(), (1, 2, 0))  # CHW to HWC\n",
    "    height, width = mask_pred_array.shape[0], mask_pred_array.shape[1]\n",
    "\n",
    "    threshold = 0.8\n",
    "    thresholded = np.zeros((height, width))\n",
    "    for row in range(height):\n",
    "        for column in range(width):\n",
    "            if mask_pred_array[row][column][0] < (1-threshold) and mask_pred_array[row][column][1] > threshold:\n",
    "                thresholded[row][column] = 1\n",
    "\n",
    "    # use a boolean condition to find where pixel values are > 0.75\n",
    "    blobs = thresholded == 1\n",
    "\n",
    "    # label connected regions that satisfy this condition\n",
    "    labels, nlabels = ndimage.label(blobs, structure=[[1,1,1],[1,1,1],[1,1,1]])\n",
    "\n",
    "\n",
    "    # find their centres of mass. in this case I'm weighting by the pixel values in\n",
    "    # `img`, but you could also pass the boolean values in `blobs` to compute the\n",
    "    # unweighted centroids.\n",
    "    r, c = np.vstack(ndimage.center_of_mass(thresholded, labels, np.arange(nlabels) + 1)).T\n",
    "\n",
    "    # find their distances from the top-left corner\n",
    "    d = np.sqrt(r*r + c*c)\n",
    "\n",
    "\n",
    "    # Get coordinates for each unique band\n",
    "    # create array of intensities\n",
    "    volume_labels = np.zeros((nlabels+1), float)\n",
    "    area_labels = np.zeros((nlabels+1), int)\n",
    "\n",
    "    for h in range(height):\n",
    "        for w in range(width):\n",
    "            volume_labels[labels[h][w]] += image[h][w]  # index = label, value += intensity(between 0 and 1)\n",
    "            area_labels[labels[h][w]] += 1\n",
    "\n",
    "    original_image = image.detach().squeeze().cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(original_image, cmap='gray')\n",
    "    plt.imshow(np.ma.masked_array(labels, ~blobs), cmap=plt.cm.rainbow)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T10:05:52.207342Z",
     "start_time": "2023-05-18T10:05:29.248612Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
