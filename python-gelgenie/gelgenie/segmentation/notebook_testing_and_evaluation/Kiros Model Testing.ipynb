{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d80a1c89",
   "metadata": {},
   "source": [
    "# Model Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b48425",
   "metadata": {},
   "source": [
    "Repo: https://github.com/qubvel/segmentation_models.pytorch\n",
    "\n",
    "Models: https://smp.readthedocs.io/en/latest/models.html#unet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294019a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "9a412f19",
   "metadata": {},
   "source": [
    "# import importlib\n",
    "# importlib.reload()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e24621cc",
   "metadata": {},
   "source": [
    "from segmentation.helper_functions.data_functions import prep_dataloader"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb9da722",
   "metadata": {},
   "source": [
    "prep_dataloader"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13a9977f",
   "metadata": {},
   "source": [
    "from segmentation_models_pytorch import utils\n",
    "import torch"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f792bd7d",
   "metadata": {},
   "source": [
    "# Create Segmentation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ddef6f0e",
   "metadata": {},
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=\"resnet18\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    # encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=2,                      # model output channels (number of classes in your dataset)\n",
    "    activation='softmax')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "690ebb2e",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb201f01",
   "metadata": {},
   "source": [
    "dir_train_img = 'C:/2022_Summer_Intern/Gel_Images_UNet_Test/Final_Set/Training_Set/Images'\n",
    "dir_train_mask = 'C:/2022_Summer_Intern/Gel_Images_UNet_Test/Final_Set/Training_Set/Masks'\n",
    "dir_val_img = 'C:/2022_Summer_Intern/Gel_Images_UNet_Test/Final_Set/Validation_Set/Images'\n",
    "dir_val_mask = 'C:/2022_Summer_Intern/Gel_Images_UNet_Test/Final_Set/Validation_Set/Masks'\n",
    "n_channels = 1\n",
    "img_scale = 0.5\n",
    "val_percent = 0.1\n",
    "batch_size = 1\n",
    "num_workers = 0\n",
    "apply_augmentations = False\n",
    "padding = True"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "345ce8aa",
   "metadata": {},
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from segmentation.helper_functions.general_functions import extract_image_names_from_folder\n",
    "import torchvision.transforms as transforms\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n",
    "class BasicDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 images_dir: str,\n",
    "                 masks_dir: str,\n",
    "                 n_channels: int,\n",
    "                 scale: float = 1.0,\n",
    "                 mask_suffix: str = '',\n",
    "                 augmentations=None,\n",
    "                 padding=False):\n",
    "        \"\"\"\n",
    "        TODO: fill in!\n",
    "        :param images_dir:\n",
    "        :param masks_dir:\n",
    "        :param n_channels:\n",
    "        :param scale:\n",
    "        :param mask_suffix:\n",
    "        \"\"\"\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.masks_dir = Path(masks_dir)\n",
    "        assert (n_channels == 1 or n_channels == 3), 'Number of channels must be either 1 or 3'\n",
    "        self.n_channels = n_channels\n",
    "        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n",
    "        self.scale = scale\n",
    "        self.mask_suffix = mask_suffix\n",
    "        self.standard_image_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "        self.image_names = extract_image_names_from_folder(images_dir)\n",
    "        self.mask_names = extract_image_names_from_folder(masks_dir)\n",
    "        self.masks_dict = {os.path.basename(mask).split('.')[0]: mask for mask in self.mask_names}\n",
    "\n",
    "        max_dimension = 0\n",
    "        for root, dirs, files in os.walk(self.images_dir):\n",
    "            for name in files:\n",
    "                image_file = os.path.join(root, name)\n",
    "                image = imageio.imread(image_file)\n",
    "                max_dimension = max(max_dimension, image.shape[0], image.shape[1])\n",
    "        max_dimension = 32*(max_dimension//32+1)\n",
    "        self.max_dimension = max_dimension\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "        self.padding = padding\n",
    "\n",
    "        if not self.image_names:\n",
    "            raise RuntimeError(f'No input file found in {images_dir}, make sure you put your images there')\n",
    "        if not self.mask_names:\n",
    "            raise RuntimeError(f'No input file found in {masks_dir}, make sure you put your images there')\n",
    "        logging.info(f'Creating dataset with {len(self.image_names)} examples')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_image(self, filename, n_channels):\n",
    "        image = imageio.imread(filename)\n",
    "\n",
    "        # Converting to desired number of channels\n",
    "        if n_channels == 1:  # Target input: 1 channel\n",
    "            if image.shape[-1] == 3:  # Actual input: 3 channels\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "            elif image.shape[-1] == 4:  # Actual input: 4 channels\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGBA2GRAY)\n",
    "            # No change required for already grayscale images\n",
    "        elif n_channels == 3:  # Target input: 3 channels\n",
    "            if image.shaoe[-1] == 4:  # Actual input: 4 channels\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "            elif image.shape[-1] != 3:  # Actual input: 1 channels\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        # Normalizing image\n",
    "        if image.dtype == 'uint8':\n",
    "            max_val = 255\n",
    "        elif image.dtype == 'uint16':\n",
    "            max_val = 65535\n",
    "        image = image.astype(np.float32) / (max_val - 0)\n",
    "\n",
    "        return image\n",
    "\n",
    "    @staticmethod\n",
    "    def load_mask(filename):\n",
    "        pil_mask = Image.open(filename)\n",
    "        final_mask = np.array(pil_mask)\n",
    "        unique = np.unique(final_mask)\n",
    "        final_mask = np.array([[np.where(unique == i)[0][0] for i in j] for j in final_mask])\n",
    "        return final_mask\n",
    "\n",
    "    # in your init function - run glob on the dataset folder, this gets all images and puts them in a list\n",
    "    # 2 when you get your id in __getitem__, just index the above list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_file = self.image_names[idx]\n",
    "        mask_file = self.masks_dict[os.path.basename(img_file).split('.')[0]]\n",
    "\n",
    "        if os.path.basename(img_file).split('.')[0] != os.path.basename(mask_file).split('.')[0]:\n",
    "            raise RuntimeError('Gel and Mask images do not match')\n",
    "\n",
    "        img_array = self.load_image(self, filename=img_file, n_channels=self.n_channels)\n",
    "        mask_array = self.load_mask(mask_file)\n",
    "\n",
    "        assert img_array.shape[0] == mask_array.shape[0] and \\\n",
    "               img_array.shape[1] == mask_array.shape[1], \\\n",
    "            f'Image and mask should be the same size, but are {img_array.shape} and {mask_array.shape}'\n",
    "\n",
    "        if self.augmentations:\n",
    "            sample = self.augmentations(image=img_array, mask=mask_array)\n",
    "            img_array = sample['image']\n",
    "            mask_array = sample['mask']\n",
    "\n",
    "        if self.padding:\n",
    "            top = (self.max_dimension - img_array.shape[0]) // 2\n",
    "            bottom = self.max_dimension - img_array.shape[0] - top\n",
    "            left = (self.max_dimension - img_array.shape[1]) // 2\n",
    "            right = self.max_dimension - img_array.shape[1] - left\n",
    "\n",
    "            img_array = np.pad(img_array, pad_width=((top, bottom), (left, right)), mode='constant')\n",
    "            mask_array = np.pad(mask_array, pad_width=((top, bottom), (left, right)), mode='constant')\n",
    "\n",
    "        img_tensor = self.standard_image_transform(img_array)\n",
    "        mask_tensor = torch.from_numpy(mask_array).int().contiguous()\n",
    "\n",
    "        return img_tensor, mask_tensor"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f09be13",
   "metadata": {},
   "source": [
    "# train_loader, val_loader, n_train, n_val = prep_dataloader(\n",
    "#     dir_train_img, dir_train_mask, dir_val_img, dir_val_mask, \n",
    "#     n_channels, img_scale, val_percent, batch_size, num_workers, \n",
    "#     apply_augmentations, padding)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "53f559ce",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_set = BasicDataset(dir_train_img, dir_train_mask, n_channels, img_scale,\n",
    "                                 augmentations=None, padding=padding)\n",
    "val_set = BasicDataset(dir_val_img, dir_val_mask, n_channels, img_scale,\n",
    "                           augmentations=None, padding=padding)\n",
    "n_train = int(len(train_set))\n",
    "n_val = int(len(val_set))\n",
    "\n",
    "# 3. Create data loaders\n",
    "loader_args = dict(batch_size=batch_size, num_workers=num_workers, pin_memory=True)\n",
    "train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
    "val_loader = DataLoader(val_set, shuffle=False, drop_last=True, batch_size=1, num_workers=1, pin_memory=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a3b06be4",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4fcc650a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
    "# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index\n",
    "\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.0001),\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1c25939",
   "metadata": {},
   "source": [
    "# create epoch runners \n",
    "# it is a simple loop of iterating over dataloader`s samples\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    verbose=True,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "77f98458",
   "metadata": {},
   "source": [
    "max_score = 0\n",
    "\n",
    "for i in range(0, 40):\n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        torch.save(model, './best_model.pth')\n",
    "        print('Model saved!')\n",
    "        \n",
    "    if i == 25:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-5\n",
    "        print('Decrease decoder learning rate to 1e-5!')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc9c461",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ec44b",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f05344c1",
   "metadata": {},
   "source": [
    "from torchinfo import summary\n",
    "model_structure = summary(model, mode='train', depth=10, device='cpu', verbose=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "add0660b",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "print(model_structure)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "61ed32d2",
   "metadata": {},
   "source": [
    "model.named_parameters()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "70d200e6",
   "metadata": {},
   "source": [
    "test=None"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ff67bb81",
   "metadata": {},
   "source": [
    "teststr = int(test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fcd304d0",
   "metadata": {},
   "source": [
    "if teststr is not None:\n",
    "    print('string')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0eb14b91",
   "metadata": {},
   "source": [
    "bool('test')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac52c05",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
