{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2b3af4bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:18.736708Z",
     "start_time": "2023-12-13T10:02:18.707459Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import re\n",
    "from gelgenie.segmentation.helper_functions.general_functions import create_dir_if_empty"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Main data prep functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8c79d087c785513"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "617d4157",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:19.604799Z",
     "start_time": "2023-12-13T10:02:19.590406Z"
    }
   },
   "source": [
    "def modify_keys_and_add_prefix(data_dict, prefix):\n",
    "    \"\"\"\n",
    "    Modify keys in a dictionary by removing a prefix and adding a new prefix.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dict (dict): The input dictionary to be modified.\n",
    "    - prefix (str): The prefix to be added to the modified keys.\n",
    "\n",
    "    Modifies the keys in the input dictionary by removing a prefix (if present) and adding a new prefix.\n",
    "    The modified dictionary is updated in-place.\n",
    "\n",
    "    Example:\n",
    "    >>> data_dict = {'old\\\\key_1': 10, 'old\\\\key_2': 20}\n",
    "    >>> modify_keys_and_add_prefix(data_dict, 'new_')\n",
    "    >>> print(data_dict)\n",
    "    {'new_key1': 10, 'new_key2': 20}\n",
    "    \"\"\"\n",
    "    # Iterate over a copy of keys to avoid changing the dictionary size during iteration\n",
    "    for old_key in list(data_dict.keys()):\n",
    "        # Find the index of the backslash character\n",
    "        index = old_key.find(\"\\\\\")\n",
    "        \n",
    "        # If the backslash is present, create a new key starting from the character after the backslash\n",
    "        if index != -1:\n",
    "            new_key = old_key[index + 1:]\n",
    "        else:\n",
    "            # If no backslash is found, use the original key\n",
    "            new_key = old_key\n",
    "        \n",
    "        # Remove any underscores after the backslash (if present)\n",
    "        new_key = new_key.split('_')[0] if '_' in new_key else new_key\n",
    "        \n",
    "        # Add the new prefix to the modified key\n",
    "        new_key = f\"{prefix}{new_key}\"\n",
    "        \n",
    "        # Update the dictionary with the modified key\n",
    "        data_dict[new_key] = data_dict.pop(old_key)\n",
    "\n",
    "\n",
    "def min_max_normalize(df, group_column, target_column, new_column_name=None):\n",
    "    \"\"\"\n",
    "    Min-Max normalize the values of a column in a Pandas DataFrame grouped by another column.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Pandas DataFrame\n",
    "    - group_column: Column used for grouping\n",
    "    - target_column: Column to be min-max normalized\n",
    "    - new_column_name: Name for the new column with normalized values (default is 'Normalized_<target_column>')\n",
    "\n",
    "    Returns:\n",
    "    - df_normalized: DataFrame with the new column containing min-max normalized values within each group\n",
    "    \"\"\"\n",
    "    # If new_column_name is not provided, create a default name\n",
    "    if new_column_name is None:\n",
    "        new_column_name = f'Normalized_{target_column}'\n",
    "\n",
    "    # Calculate the min and max values for each group\n",
    "    min_values = df.groupby(group_column)[target_column].transform('min')\n",
    "    max_values = df.groupby(group_column)[target_column].transform('max')\n",
    "\n",
    "    # Apply the min-max normalization formula and add a new column\n",
    "    df[new_column_name] = (df[target_column] - min_values) / (max_values - min_values)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_ga_csv_files_from_folders(parent_folder, prefix=\"ga_\"):\n",
    "    \"\"\"\n",
    "    Load CSV files from folders and create a dictionary of DataFrames with prefixed names.\n",
    "\n",
    "    Parameters:\n",
    "    - parent_folder (str): The path to the parent folder containing numbered subfolders.\n",
    "    - prefix (str): The prefix to be added to the names of the DataFrames (default is \"ga_\").\n",
    "\n",
    "    Returns:\n",
    "    - dataframes (dict): A dictionary where keys are prefixed folder names, and values are corresponding DataFrames.\n",
    "    \"\"\"\n",
    "    dataframes = {}  # Dictionary to store DataFrames\n",
    "\n",
    "    # Iterate through folders in the parent folder\n",
    "    for folder_name in os.listdir(parent_folder):\n",
    "        folder_path = os.path.join(parent_folder, folder_name)\n",
    "\n",
    "        # Check if the item in the parent folder is a directory\n",
    "        if os.path.isdir(folder_path):\n",
    "            csv_file_path = os.path.join(folder_path, \"collated_data_with_band_quality.csv\")\n",
    "\n",
    "            # Check if \"collated_data_with_band_quality.csv\" exists in the folder\n",
    "            if os.path.isfile(csv_file_path):\n",
    "                # Read the CSV file into a DataFrame\n",
    "                dataframe = pd.read_csv(csv_file_path)\n",
    "\n",
    "                # Add the prefix to the folder name and use it as the key in the dictionary\n",
    "                prefixed_folder_name = f\"{prefix}{folder_name}\"\n",
    "                dataframes[prefixed_folder_name] = dataframe\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "def min_max_normalize_multiple_inplace(dataframes_dict, group_column, target_columns, new_column_prefix=None):\n",
    "    \"\"\"\n",
    "    Min-Max normalize the values of specified columns in multiple DataFrames stored in a dictionary.\n",
    "    Add normalized columns to the existing DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes_dict (dict): A dictionary where keys are DataFrame names and values are DataFrames.\n",
    "    - group_column (str): Column used for grouping in each DataFrame.\n",
    "    - target_columns (list): List of columns to be min-max normalized in each DataFrame.\n",
    "    - new_column_prefix (str): Prefix for the new columns with normalized values (default is 'Normalized_').\n",
    "\n",
    "    Modifies the input DataFrames in-place by adding normalized columns.\n",
    "\n",
    "    Example:\n",
    "    >>> dataframes_dict = {'df1': pd.DataFrame({'Group': ['A', 'A', 'B', 'B'], 'Values1': [10, 15, 5, 8]}),\n",
    "                           'df2': pd.DataFrame({'Group': ['C', 'C', 'D', 'D'], 'Values1': [50, 55, 45, 60]})}\n",
    "    >>> min_max_normalize_multiple_inplace(dataframes_dict, group_column='Group', target_columns=['Values1'])\n",
    "    >>> print(dataframes_dict['df1'])\n",
    "       Group  Values1  Normalized_Values1\n",
    "    0     A       10                0.00\n",
    "    1     A       15                1.00\n",
    "    2     B        5                0.00\n",
    "    3     B        8                1.00\n",
    "    \"\"\"\n",
    "    # If new_column_prefix is not provided, create a default name\n",
    "    if new_column_prefix is None:\n",
    "        new_column_prefix = 'Normalized_'\n",
    "\n",
    "    # Iterate through DataFrames in the dictionary\n",
    "    for df_name, df in dataframes_dict.items():\n",
    "        # Iterate through target columns in each DataFrame\n",
    "        for target_column in target_columns:\n",
    "            # Calculate the min and max values for each group in the current DataFrame\n",
    "            min_values = df.groupby(group_column)[target_column].transform('min')\n",
    "            max_values = df.groupby(group_column)[target_column].transform('max')\n",
    "\n",
    "            # Add a new column with the normalized values to the existing DataFrame\n",
    "            new_column_name = f\"{new_column_prefix}{target_column}\"\n",
    "            df[new_column_name] = (df[target_column] - min_values) / (max_values - min_values)\n",
    "\n",
    "        # Update the dictionary with the modified DataFrame\n",
    "        dataframes_dict[df_name] = df\n",
    "\n",
    "        \n",
    "def load_gg_csv_files_to_dict(folder_path, prefix=\"prefix_\"):\n",
    "    \"\"\"\n",
    "    Load CSV files from a folder and create a dictionary of DataFrames with prefixed keys.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): The path to the folder containing CSV files.\n",
    "    - prefix (str): The prefix to be added to the keys of the dictionary (default is \"prefix_\").\n",
    "\n",
    "    Returns:\n",
    "    - dataframes_dict (dict): A dictionary where keys are prefixed numbers, and values are corresponding DataFrames.\n",
    "    \"\"\"\n",
    "    dataframes_dict = {}  # Dictionary to store DataFrames\n",
    "\n",
    "    # Iterate through files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file is a CSV file\n",
    "        if filename.endswith(\".csv\"):\n",
    "            # Extract the number from the filename\n",
    "            file_number = filename.split(\"_\")[0]\n",
    "\n",
    "            # Read the CSV file into a DataFrame\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            dataframe = pd.read_csv(file_path)\n",
    "\n",
    "            # Add the prefix to the number and use it as the key in the dictionary\n",
    "            key = f\"{prefix}{file_number}\"\n",
    "            dataframes_dict[key] = dataframe\n",
    "\n",
    "    return dataframes_dict\n",
    "\n",
    "\n",
    "def remove_spaces_from_column_headings_inplace(dataframes_dict):\n",
    "    \"\"\"\n",
    "    Remove spaces at the start and end of column headings in pandas DataFrames stored in a dictionary.\n",
    "    Replace the old columns with the modified column headings in-place.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes_dict (dict): A dictionary where keys are DataFrame names and values are DataFrames.\n",
    "\n",
    "    Modifies the input DataFrames in-place by replacing old columns with modified column headings.\n",
    "    \"\"\"\n",
    "    # Iterate through DataFrames in the dictionary\n",
    "    for df_name, df in dataframes_dict.items():\n",
    "        # Remove spaces at the start and end of column headings and replace the old columns\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        # Update the dictionary with the modified DataFrame\n",
    "        dataframes_dict[df_name] = df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f405b29f92ee6ab"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eb8ec275",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:23.001405Z",
     "start_time": "2023-12-13T10:02:22.719840Z"
    }
   },
   "source": [
    "# modify the below paths to point to wherever the gelgenie (gg)/gelanalyzer (ga)/ reference data is stored\n",
    "gg_path = Path(\"/Users/matt/Documents/PhD/research_output/Automatic_Gel_Analyzer/quantitative_results/qupath_data/james_data_v3_fixed_global/Data\")\n",
    "ga_path = Path(\"/Users/matt/Documents/PhD/research_output/Automatic_Gel_Analyzer/quantitative_results/gelanalyzer\")\n",
    "reference_path = Path(\"/Users/matt/Documents/PhD/research_output/Automatic_Gel_Analyzer/quantitative_results/reference_ladder_masses.csv\")\n",
    "\n",
    "gg_dfs = load_gg_csv_files_to_dict(gg_path, \"gg_\") # loads data and converts to dictionary\n",
    "remove_spaces_from_column_headings_inplace(gg_dfs) # deletes spaces from column headings (due to an old bug that has now been fixed)\n",
    "\n",
    "gg_dfs = {key: gg_dfs[key] for key in sorted(gg_dfs.keys(), key=lambda s: [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', s)])}  # sorts by gel ID\n",
    "\n",
    "ga_dfs = load_ga_csv_files_from_folders(ga_path)\n",
    "ga_dfs = {key: ga_dfs[key] for key in sorted(ga_dfs.keys(), key=lambda s: [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', s)])} # sorts by gel ID\n",
    "\n",
    "for key, df in ga_dfs.items():\n",
    "    ga_dfs[key] = df[df['Reliable Band'] != 0].copy() # removes all band data that has been deemed as unreliable\n",
    "\n",
    "reference_df = pd.read_csv(reference_path)\n",
    "reference_df.rename(columns={\"NEB ladder\": \"NEB\", \" ThermoFisher ladder\": \"Thermo\"}, inplace=True)\n",
    "reference_df[\"Band ID\"] = range(1, len(reference_df) + 1)\n",
    "reference_df = pd.melt(reference_df, id_vars=[\"Band ID\"], value_vars=[\"NEB\", \"Thermo\"], var_name=\"Ladder\", value_name=\"Intensity\")\n",
    "reference_df['Normalized_Intensity'] = reference_df.groupby(\"Ladder\")[\"Intensity\"].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "print('reference data:')\n",
    "reference_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "print('Example gelgenie data:')\n",
    "gg_dfs['gg_0']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:24.321544Z",
     "start_time": "2023-12-13T10:02:24.177424Z"
    }
   },
   "id": "ab4d4474e3c732c2",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "print('Example gelanalyzer data:')\n",
    "ga_dfs['ga_31']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:25.706967Z",
     "start_time": "2023-12-13T10:02:25.670360Z"
    }
   },
   "id": "78f92f38226463c6",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Normalization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9de7cebd08f0b78"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d8035e45",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:28.207713Z",
     "start_time": "2023-12-13T10:02:27.941277Z"
    }
   },
   "source": [
    "ga_columns = ['Raw Volume', 'Background Corrected Volume']\n",
    "gg_columns = ['Raw Volume','Local Corrected Volume', 'Global Corrected Volume']\n",
    "\n",
    "min_max_normalize_multiple_inplace(ga_dfs, 'Lane ID', ga_columns)  \n",
    "min_max_normalize_multiple_inplace(gg_dfs, 'Lane ID', gg_columns)\n",
    "\n",
    "normalized_gg_columns = ['Normalized_Raw Volume', 'Normalized_Local Corrected Volume', 'Normalized_Global Corrected Volume']\n",
    "normalized_ga_columns = ['Normalized_Raw Volume', 'Normalized_Background Corrected Volume']"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Concatenation and final DF generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f93b670246530ca8"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f0ac9d5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:29.443988Z",
     "start_time": "2023-12-13T10:02:29.250495Z"
    }
   },
   "source": [
    "for key, df in gg_dfs.items():\n",
    "    df['App'] = \"GG\"\n",
    "    df['Gel ID'] = key\n",
    "    \n",
    "for key, df in ga_dfs.items():\n",
    "    df['App'] = \"GA\"\n",
    "    df['Gel ID'] = key\n",
    "    \n",
    "concatenated_df = pd.concat([df for df in gg_dfs.values()] + [df for df in ga_dfs.values()], ignore_index=True)\n",
    "concatenated_df['Gel ID'] = concatenated_df['Gel ID'].str.extract(r'_(\\d+)')\n",
    "concatenated_df['Gel ID'] = concatenated_df['Gel ID'].astype(int)\n",
    "concatenated_df['Ladder'] = \"Temp\"\n",
    "concatenated_df.loc[concatenated_df[\"Gel ID\"].isin([0,1,2,3,4,5,6,7,8,9,10,11,12,32,34]), \"Ladder\"] = \"Thermo\"\n",
    "concatenated_df.loc[concatenated_df[\"Gel ID\"].isin([13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,29,30,31,33]), \"Ladder\"] = \"NEB\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "concatenated_df[concatenated_df['Gel ID'] == 32]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:31.037885Z",
     "start_time": "2023-12-13T10:02:30.856511Z"
    }
   },
   "id": "f9efe880e1e99063",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f7f8ef29",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:34.648958Z",
     "start_time": "2023-12-13T10:02:34.034334Z"
    }
   },
   "source": [
    "sns.violinplot(data=concatenated_df, x='Lane ID', y='Normalized_Raw Volume', hue='App', split=True, inner='quart')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b3a8b80c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:36.944381Z",
     "start_time": "2023-12-13T10:02:36.833046Z"
    }
   },
   "source": [
    "melted_df = pd.melt(concatenated_df, id_vars=['Lane ID', 'Band ID', 'Gel ID', 'App'],\n",
    "                    value_vars=['Pixel Count', 'Average Intensity', 'Raw Volume', 'Local Corrected Volume',\n",
    "                                'Global Corrected Volume', 'Normalized_Raw Volume',\n",
    "                                'Normalized_Local Corrected Volume', 'Normalized_Global Corrected Volume',\n",
    "                                'Background Corrected Volume', 'Normalized_Background Corrected Volume'],\n",
    "                    var_name='Variable', value_name='Volume')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "melted_df # this df contains all the data split into rows for every single variable e.g. there is a single row for a normalised background value of a single band ID and lane ID.  The descriptor is held in the Variable column and the value is held in the Volumn column"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:38.776456Z",
     "start_time": "2023-12-13T10:02:38.704806Z"
    }
   },
   "id": "2d9cc9e3b1a862e2",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Violin: General vs Local Background Correction (GG)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20af851877a1dc8f"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "66873c96",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:03:02.363915Z",
     "start_time": "2023-12-13T10:02:41.343091Z"
    }
   },
   "source": [
    "# Select rows based on the 'Variable' column\n",
    "selected_variables = ['Normalized_Local Corrected Volume', 'Normalized_Global Corrected Volume']\n",
    "#additional_column_value = 'GA'  # Replace with the specific value you want to filter on\n",
    "subset_df = melted_df[(melted_df['Variable'].isin(selected_variables))]\n",
    "#subset_df = melted_df[(melted_df['Variable'].isin(selected_variables)) & (melted_df['App'] == additional_column_value)]\n",
    "\n",
    "# Get unique values in the \"Gel ID\" column\n",
    "unique_gel_ids = concatenated_df['Gel ID'].unique()\n",
    "\n",
    "output_dir = '/Users/matt/Desktop/violin_localvsglobal'\n",
    "\n",
    "create_dir_if_empty(output_dir)\n",
    "\n",
    "for gel_id in unique_gel_ids:\n",
    "    subset_df = melted_df[(melted_df['Variable'].isin(selected_variables)) & (melted_df['Gel ID'] == gel_id)]\n",
    "\n",
    "    # Plot the violin plot with log scale on the y-axis\n",
    "    sns.violinplot(data=subset_df, x='Lane ID', y='Volume', hue='Variable', split=True, inner='quart')\n",
    "\n",
    "    plt.title(f\"Gel - {gel_id}\")\n",
    "    plt.savefig(os.path.join(output_dir, f\"gel_{gel_id}.jpg\"))\n",
    "    plt.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Violin: GA vs GG (raw)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "319e0037dc9b177b"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "040ff258",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:03:15.616317Z",
     "start_time": "2023-12-13T10:03:02.373009Z"
    }
   },
   "source": [
    "# Select rows based on the 'Variable' column\n",
    "selected_variables = ['Normalized_Raw Volume']\n",
    "#additional_column_value = 'GA'  # Replace with the specific value you want to filter on\n",
    "subset_df = melted_df[(melted_df['Variable'].isin(selected_variables))]\n",
    "#subset_df = melted_df[(melted_df['Variable'].isin(selected_variables)) & (melted_df['App'] == additional_column_value)]\n",
    "\n",
    "# Get unique values in the \"Gel ID\" column\n",
    "unique_gel_ids = concatenated_df['Gel ID'].unique()\n",
    "\n",
    "output_dir = '/Users/matt/Desktop/violin_raw_ggvsga'\n",
    "\n",
    "create_dir_if_empty(output_dir)\n",
    "\n",
    "for gel_id in unique_gel_ids:\n",
    "    \n",
    "    subset_df = melted_df[(melted_df['Variable'].isin(selected_variables)) & (melted_df['Gel ID'] == gel_id)]\n",
    "    if 'GA' not in subset_df['App'].unique():\n",
    "        continue\n",
    "\n",
    "    # Plot the violin plot with log scale on the y-axis\n",
    "    sns.violinplot(data=subset_df, x='Lane ID', y='Volume', hue='App', split=True, inner='quart', palette='muted')\n",
    "\n",
    "    plt.title(f\"Gel - {gel_id} - Normalised Raw Volume\")\n",
    "    plt.savefig(os.path.join(output_dir, f\"gel_{gel_id}.jpg\"))\n",
    "    plt.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Violin: Comparing all background correction methods"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cf9ece5b8923002"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e884feb9",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:03:30.302968Z",
     "start_time": "2023-12-13T10:03:15.602138Z"
    }
   },
   "source": [
    "# Select rows based on the 'Variable' column\n",
    "selected_variables = ['Normalized_Local Corrected Volume', 'Normalized_Global Corrected Volume', 'Normalized_Background Corrected Volume']\n",
    "#additional_column_value = 'GA'  # Replace with the specific value you want to filter on\n",
    "subset_df = melted_df[(melted_df['Variable'].isin(selected_variables))]\n",
    "#subset_df = melted_df[(melted_df['Variable'].isin(selected_variables)) & (melted_df['App'] == additional_column_value)]\n",
    "\n",
    "# Get unique values in the \"Gel ID\" column\n",
    "unique_gel_ids = concatenated_df['Gel ID'].unique()\n",
    "\n",
    "output_dir = '/Users/matt/Desktop/violin_background_correction'\n",
    "create_dir_if_empty(output_dir)\n",
    "\n",
    "for gel_id in unique_gel_ids:\n",
    "    subset_df = melted_df[(melted_df['Variable'].isin(selected_variables)) & (melted_df['Gel ID'] == gel_id)]\n",
    "    if 'GA' not in subset_df['App'].unique():\n",
    "        continue\n",
    "    # Plot the violin plot with log scale on the y-axis\n",
    "    sns.violinplot(data=subset_df, x='Lane ID', y='Volume', hue='Variable', palette='Set2', inner='quart')\n",
    "\n",
    "    plt.title(f\"Gel - {gel_id} - Normalised Volumes\")\n",
    "    plt.savefig(os.path.join(output_dir, f\"gel_{gel_id}.jpg\"))\n",
    "    plt.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analysis of error vs reference ladder values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d9d346095c3031a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Merging in reference values into main dataframe"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60e846d09a0063a8"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7714e136",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:03:30.349986Z",
     "start_time": "2023-12-13T10:03:30.308612Z"
    }
   },
   "source": [
    "merged_df = pd.merge(concatenated_df, reference_df, on=['Band ID', 'Ladder'], how='left')\n",
    "merged_df['Expected Value'] = merged_df['Normalized_Intensity']\n",
    "merged_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f26d6181",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:03:30.376746Z",
     "start_time": "2023-12-13T10:03:30.335940Z"
    }
   },
   "source": [
    "# simple error used here\n",
    "merged_df[\"Raw Difference\"] = merged_df[\"Normalized_Raw Volume\"] - merged_df['Expected Value']\n",
    "merged_df[\"Local Difference\"] = merged_df[\"Normalized_Local Corrected Volume\"] - merged_df['Expected Value']\n",
    "merged_df[\"Global Difference\"] = merged_df[\"Normalized_Global Corrected Volume\"] - merged_df['Expected Value']\n",
    "merged_df[\"Background Difference\"] = merged_df[\"Normalized_Background Corrected Volume\"] - merged_df['Expected Value']\n",
    "merged_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Min-Max Error plotting: Raw Volumes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0c193c9b95f0c73"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "output_dir = '/Users/matt/Desktop/pdf_error_plots'\n",
    "create_dir_if_empty(output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:03:30.396076Z",
     "start_time": "2023-12-13T10:03:30.372653Z"
    }
   },
   "id": "4f914a71f8c44939",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "930e0b9a",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:03:53.889835Z",
     "start_time": "2023-12-13T10:03:30.386595Z"
    }
   },
   "source": [
    "# plots the minimum and maximum raw error for each band ID in a gel (over all lanes in the specific gel)\n",
    "\n",
    "# Output PDF file path\n",
    "output_pdf_path = join(output_dir,'gavsgg_raw.pdf')\n",
    "\n",
    "# Iterate over unique Gel IDs in merged_df and create separate plots\n",
    "unique_gel_ids = merged_df['Gel ID'].unique()\n",
    "\n",
    "with PdfPages(output_pdf_path) as pdf:\n",
    "    for gel_id in unique_gel_ids:\n",
    "        # Filter DataFrame for the current Gel ID\n",
    "        subset_df = merged_df[merged_df['Gel ID'] == gel_id]\n",
    "        \n",
    "        # Extract unique Band IDs\n",
    "        unique_band_ids = subset_df['Band ID'].unique()\n",
    "        \n",
    "        # Plot the results\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # List to store legends\n",
    "        legends = []\n",
    "        \n",
    "        # Iterate over unique values in the 'App' column\n",
    "        for app_value in subset_df['App'].unique():\n",
    "            # Calculate minimum and maximum Raw Difference per Band ID for each App\n",
    "            min_max_diff = subset_df[subset_df['App'] == app_value].groupby('Band ID')['Raw Difference'].agg(['min', 'max']).reset_index()\n",
    "            \n",
    "            # Plot lines for each App\n",
    "            for j, (band_id, row) in enumerate(min_max_diff.iterrows()):\n",
    "                # Choose linestyle and marker based on the 'App' column\n",
    "                linestyle = '--' if app_value == 'GA' else '-'\n",
    "                marker = 'o' if app_value == 'GG' else '^'\n",
    "                \n",
    "                # Set color based on 'App' column\n",
    "                color = 'red' if app_value == 'GG' else 'blue'\n",
    "                \n",
    "                line, = plt.plot(\n",
    "                    [row['min'], row['max']],\n",
    "                    [band_id, band_id],\n",
    "                    linestyle=linestyle,\n",
    "                    alpha=0.5,  # Set alpha to 0.5 for transparency\n",
    "                    marker=marker,\n",
    "                    color=color,\n",
    "                    label=f'{app_value} - Band ID {row[\"Band ID\"]}',\n",
    "                )\n",
    "                legends.append(line)\n",
    "        \n",
    "        plt.yticks(range(len(unique_band_ids)), unique_band_ids)  # Set y-axis ticks to unique Band IDs\n",
    "        plt.title(f'Min and Max Raw Difference per Band ID - Gel ID {gel_id}')\n",
    "        plt.xlabel('Raw Difference')\n",
    "        plt.ylabel('Band ID')\n",
    "        \n",
    "        # Add a green dashed vertical line at point 0\n",
    "        plt.axvline(x=0, color='green', linestyle='--', label='Zero Line')\n",
    "        \n",
    "        # Place the legend outside the plot to the right\n",
    "        plt.legend(handles=legends + [plt.Line2D([0], [0], color='green', linestyle='--')], bbox_to_anchor=(1.05, 1), loc='upper left', title='App')\n",
    "        \n",
    "        # Save the plot to the PDF file\n",
    "        pdf.savefig(bbox_inches='tight')  # Use bbox_inches='tight' to include the legend\n",
    "        plt.close()\n",
    "\n",
    "print(f'Plots saved to {output_pdf_path}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "list(merged_df['Band ID'].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:03:53.895945Z",
     "start_time": "2023-12-13T10:03:53.889226Z"
    }
   },
   "id": "be08c7b22a76290d",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Min-Max Error plotting: Custom Values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8b8c9e565f7c52"
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "392dfe29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:03:53.910998Z",
     "start_time": "2023-12-13T10:03:53.895268Z"
    }
   },
   "source": [
    "# function that allows you to select which error quantities should be plotted out (up to 3 max)\n",
    "def plot_min_max_values_to_pdf(df, group_columns, output_pdf_path='output_plots.pdf', mode='min_max'):\n",
    "    # Extract unique Gel IDs\n",
    "    unique_gel_ids = df['Gel ID'].unique()\n",
    "    # Assign different markers and colors to the columns\n",
    "    markers = ['o', '^', '*']\n",
    "    colors = ['red', 'blue', 'orange']\n",
    "    linestyle = '--'\n",
    "    \n",
    "    with PdfPages(output_pdf_path) as pdf:\n",
    "        for gel_id in unique_gel_ids:\n",
    "            # Filter DataFrame for the current Gel ID\n",
    "            subset_df = df[df['Gel ID'] == gel_id]\n",
    "\n",
    "            # Extract unique Band IDs\n",
    "            unique_band_ids = subset_df['Band ID'].unique()\n",
    "\n",
    "            # Plot the results for each Gel ID\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            legends = []\n",
    "            for i, band_id in enumerate(unique_band_ids):\n",
    "                # Filter DataFrame for the current Band ID\n",
    "                band_df = subset_df[subset_df['Band ID'] == band_id]   \n",
    "                for j, column in enumerate(group_columns):\n",
    "                    if mode == 'min_max':\n",
    "                        # Calculate minimum and maximum values for the current column\n",
    "                        min_value = band_df[column].min()\n",
    "                        max_value = band_df[column].max()\n",
    "    \n",
    "                        # Plot lines for each column\n",
    "                        line, = plt.plot(\n",
    "                            [min_value, max_value],\n",
    "                            [band_id, band_id],\n",
    "                            linestyle=linestyle,\n",
    "                            alpha=0.5,\n",
    "                            marker=markers[j],\n",
    "                            color=colors[j],\n",
    "                            label=f'{column} - Band ID {band_id}',\n",
    "                        )\n",
    "                    elif mode == 'mean_std':\n",
    "                        # Calculate average value for the current column\n",
    "                        avg_value = band_df[column].mean()\n",
    "            \n",
    "                        # Calculate standard deviation for error bars\n",
    "                        std_dev = band_df[column].std()\n",
    "   \n",
    "                        # Plot lines and error bars for each column\n",
    "                        line = plt.errorbar(\n",
    "                            avg_value,\n",
    "                            band_id,\n",
    "                            xerr=std_dev,\n",
    "                            linestyle=linestyle,\n",
    "                            alpha=0.5,\n",
    "                            marker=markers[j],\n",
    "                            color=colors[j],\n",
    "                            label=f'{column} - Band ID {band_id}',\n",
    "                        )\n",
    "                    elif mode == 'mean':\n",
    "                        # Calculate minimum and maximum values for the current column\n",
    "                        avg_value = band_df[column].mean()\n",
    "    \n",
    "                        # Plot lines for each column\n",
    "                        line, = plt.plot(\n",
    "                            avg_value,\n",
    "                            band_id,\n",
    "                            linestyle=linestyle,\n",
    "                            alpha=0.5,\n",
    "                            marker=markers[j],\n",
    "                            color=colors[j],\n",
    "                            label=f'{column} - Band ID {band_id}',\n",
    "                        )\n",
    "                    if band_id == 1:\n",
    "                        legends.append(line)\n",
    "\n",
    "            plt.title(f'Gel ID {gel_id}')\n",
    "            plt.xlabel('Values')\n",
    "            plt.ylabel('Band ID')\n",
    "\n",
    "            # Add a green dashed vertical line at point 0\n",
    "            plt.axvline(x=0, color='green', linestyle='--', label='Zero Line')\n",
    "\n",
    "            # Place the legend outside the plot to the right\n",
    "            plt.legend(handles=legends + [plt.Line2D([0], [0], color='green', linestyle='--')], bbox_to_anchor=(1.05, 1), loc='upper left', title='Legend')\n",
    "\n",
    "            # Save the plot to the PDF file\n",
    "            pdf.savefig(bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "    print(\"PDF Saved\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming merged_df is your DataFrame\n",
    "# plot_min_max_values_to_pdf(merged_df, group_columns=['Local Difference', 'Global Difference'], output_pdf_path='output_plots.pdf')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a4ed5292",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:04:08.730153Z",
     "start_time": "2023-12-13T10:03:53.913024Z"
    }
   },
   "source": [
    "# gg only\n",
    "plot_min_max_values_to_pdf(merged_df, group_columns=['Local Difference', 'Global Difference'], output_pdf_path=join(output_dir,'minmax_gg_background_correction_error.pdf'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f518e2b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:04:23.036733Z",
     "start_time": "2023-12-13T10:04:08.731814Z"
    }
   },
   "source": [
    "# gg global vs ga\n",
    "plot_min_max_values_to_pdf(merged_df, group_columns=['Global Difference', 'Background Difference'], output_pdf_path=join(output_dir,'minmax_gg_vs_ga_global.pdf'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "077a3188",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:04:23.055037Z",
     "start_time": "2023-12-13T10:04:23.038672Z"
    }
   },
   "source": [
    "gg_rows = merged_df[merged_df['App'] == 'GG']\n",
    "ga_rows = merged_df[merged_df['App'] == 'GA']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "876b6c54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:04:35.630390Z",
     "start_time": "2023-12-13T10:04:23.047705Z"
    }
   },
   "source": [
    "# ga raw vs background\n",
    "plot_min_max_values_to_pdf(ga_rows, group_columns=['Raw Difference', 'Background Difference'], output_pdf_path=join(output_dir,'ga_raw_background.pdf'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8dbe6d6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:04:49.984779Z",
     "start_time": "2023-12-13T10:04:35.632366Z"
    }
   },
   "source": [
    "# gg local vs raw\n",
    "plot_min_max_values_to_pdf(gg_rows, group_columns=['Raw Difference', 'Local Difference'], output_pdf_path=join(output_dir,'gg_raw-local.pdf'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f2fc8c5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:05:04.096641Z",
     "start_time": "2023-12-13T10:04:49.982493Z"
    }
   },
   "source": [
    "# gg global vs raw\n",
    "plot_min_max_values_to_pdf(gg_rows, group_columns=['Raw Difference', 'Global Difference'], output_pdf_path=join(output_dir,'gg_raw-global.pdf'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "# all background correction methods\n",
    "plot_min_max_values_to_pdf(merged_df, group_columns=['Background Difference', 'Global Difference', 'Local Difference'], output_pdf_path=join(output_dir,'minmax_all_background_errors.pdf'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:05:19.423035Z",
     "start_time": "2023-12-13T10:05:04.097525Z"
    }
   },
   "id": "e07a3f50ad75303b",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "# all background correction methods\n",
    "plot_min_max_values_to_pdf(merged_df, group_columns=['Background Difference', 'Global Difference', 'Local Difference'], output_pdf_path=join(output_dir,'mean_std_all_background_methods.pdf'),mode='mean_std')\n",
    "plot_min_max_values_to_pdf(merged_df, group_columns=['Background Difference', 'Global Difference', 'Local Difference'], output_pdf_path=join(output_dir,'mean_all_background_methods.pdf'),mode='mean')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:05:57.323518Z",
     "start_time": "2023-12-13T10:05:19.422456Z"
    }
   },
   "id": "3fc676b7727f6057",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "merged_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:07:56.110313Z",
     "start_time": "2023-12-13T10:07:56.042531Z"
    }
   },
   "id": "fd9968a0cfa8d54d",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "merged_df.to_csv(join(output_dir,'merged_df.csv'),index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:19:51.252948Z",
     "start_time": "2023-12-13T10:19:50.956095Z"
    }
   },
   "id": "4021eaee2d006242",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f6a6d3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Split the DataFrame based on 'App' values\n",
    "df_gg = merged_df[merged_df['App'] == 'GG'].copy()\n",
    "df_ga = merged_df[merged_df['App'] == 'GA'].copy()\n",
    "\n",
    "# Rename the 'Raw Difference' column in each filtered DataFrame\n",
    "df_gg.rename(columns={'Raw Difference': 'Raw Difference_GG'}, inplace=True)\n",
    "df_ga.rename(columns={'Raw Difference': 'Raw Difference_GA'}, inplace=True)\n",
    "\n",
    "# Merge the filtered DataFrames back together using outer join\n",
    "result_df = pd.merge(df_gg[['Lane ID', 'Band ID', 'Raw Difference_GG']], df_ga[['Lane ID', 'Band ID', 'Raw Difference_GA']], on=['Lane ID', 'Band ID'], how='outer')\n",
    "\n",
    "# Merge the combined result with the original DataFrame\n",
    "result_df = pd.merge(merged_df, result_df, on=['Lane ID', 'Band ID'], how='left')\n",
    "\n",
    "# Displaying the modified DataFrame\n",
    "print(result_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d6604d",
   "metadata": {},
   "source": [
    "print(merged_df.columns.tolist())"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
