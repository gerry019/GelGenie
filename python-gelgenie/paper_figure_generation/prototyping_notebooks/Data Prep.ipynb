{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3af4bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:18.736708Z",
     "start_time": "2023-12-13T10:02:18.707459Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import re\n",
    "from gelgenie.segmentation.helper_functions.general_functions import create_dir_if_empty"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b8c79d087c785513",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Main data prep functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "617d4157",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:19.604799Z",
     "start_time": "2023-12-13T10:02:19.590406Z"
    }
   },
   "source": [
    "def modify_keys_and_add_prefix(data_dict, prefix):\n",
    "    \"\"\"\n",
    "    Modify keys in a dictionary by removing a prefix and adding a new prefix.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dict (dict): The input dictionary to be modified.\n",
    "    - prefix (str): The prefix to be added to the modified keys.\n",
    "\n",
    "    Modifies the keys in the input dictionary by removing a prefix (if present) and adding a new prefix.\n",
    "    The modified dictionary is updated in-place.\n",
    "\n",
    "    Example:\n",
    "    >>> data_dict = {'old\\\\key_1': 10, 'old\\\\key_2': 20}\n",
    "    >>> modify_keys_and_add_prefix(data_dict, 'new_')\n",
    "    >>> print(data_dict)\n",
    "    {'new_key1': 10, 'new_key2': 20}\n",
    "    \"\"\"\n",
    "    # Iterate over a copy of keys to avoid changing the dictionary size during iteration\n",
    "    for old_key in list(data_dict.keys()):\n",
    "        # Find the index of the backslash character\n",
    "        index = old_key.find(\"\\\\\")\n",
    "        \n",
    "        # If the backslash is present, create a new key starting from the character after the backslash\n",
    "        if index != -1:\n",
    "            new_key = old_key[index + 1:]\n",
    "        else:\n",
    "            # If no backslash is found, use the original key\n",
    "            new_key = old_key\n",
    "        \n",
    "        # Remove any underscores after the backslash (if present)\n",
    "        new_key = new_key.split('_')[0] if '_' in new_key else new_key\n",
    "        \n",
    "        # Add the new prefix to the modified key\n",
    "        new_key = f\"{prefix}{new_key}\"\n",
    "        \n",
    "        # Update the dictionary with the modified key\n",
    "        data_dict[new_key] = data_dict.pop(old_key)\n",
    "\n",
    "\n",
    "def min_max_normalize(df, group_column, target_column, new_column_name=None):\n",
    "    \"\"\"\n",
    "    Min-Max normalize the values of a column in a Pandas DataFrame grouped by another column.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Pandas DataFrame\n",
    "    - group_column: Column used for grouping\n",
    "    - target_column: Column to be min-max normalized\n",
    "    - new_column_name: Name for the new column with normalized values (default is 'Normalized_<target_column>')\n",
    "\n",
    "    Returns:\n",
    "    - df_normalized: DataFrame with the new column containing min-max normalized values within each group\n",
    "    \"\"\"\n",
    "    # If new_column_name is not provided, create a default name\n",
    "    if new_column_name is None:\n",
    "        new_column_name = f'Normalized_{target_column}'\n",
    "\n",
    "    # Calculate the min and max values for each group\n",
    "    min_values = df.groupby(group_column)[target_column].transform('min')\n",
    "    max_values = df.groupby(group_column)[target_column].transform('max')\n",
    "\n",
    "    # Apply the min-max normalization formula and add a new column\n",
    "    df[new_column_name] = (df[target_column] - min_values) / (max_values - min_values)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_ga_csv_files_from_folders(parent_folder, prefix=\"ga_\"):\n",
    "    \"\"\"\n",
    "    Load CSV files from folders and create a dictionary of DataFrames with prefixed names.\n",
    "\n",
    "    Parameters:\n",
    "    - parent_folder (str): The path to the parent folder containing numbered subfolders.\n",
    "    - prefix (str): The prefix to be added to the names of the DataFrames (default is \"ga_\").\n",
    "\n",
    "    Returns:\n",
    "    - dataframes (dict): A dictionary where keys are prefixed folder names, and values are corresponding DataFrames.\n",
    "    \"\"\"\n",
    "    dataframes = {}  # Dictionary to store DataFrames\n",
    "\n",
    "    # Iterate through folders in the parent folder\n",
    "    for folder_name in os.listdir(parent_folder):\n",
    "        folder_path = os.path.join(parent_folder, folder_name)\n",
    "\n",
    "        # Check if the item in the parent folder is a directory\n",
    "        if os.path.isdir(folder_path):\n",
    "            csv_file_path = os.path.join(folder_path, \"collated_data_with_band_quality.csv\")\n",
    "\n",
    "            # Check if \"collated_data_with_band_quality.csv\" exists in the folder\n",
    "            if os.path.isfile(csv_file_path):\n",
    "                # Read the CSV file into a DataFrame\n",
    "                dataframe = pd.read_csv(csv_file_path)\n",
    "\n",
    "                # Add the prefix to the folder name and use it as the key in the dictionary\n",
    "                prefixed_folder_name = f\"{prefix}{folder_name}\"\n",
    "                dataframes[prefixed_folder_name] = dataframe\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "def min_max_normalize_multiple_inplace(dataframes_dict, group_column, target_columns, new_column_prefix=None):\n",
    "    \"\"\"\n",
    "    Min-Max normalize the values of specified columns in multiple DataFrames stored in a dictionary.\n",
    "    Add normalized columns to the existing DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes_dict (dict): A dictionary where keys are DataFrame names and values are DataFrames.\n",
    "    - group_column (str): Column used for grouping in each DataFrame.\n",
    "    - target_columns (list): List of columns to be min-max normalized in each DataFrame.\n",
    "    - new_column_prefix (str): Prefix for the new columns with normalized values (default is 'Normalized_').\n",
    "\n",
    "    Modifies the input DataFrames in-place by adding normalized columns.\n",
    "\n",
    "    Example:\n",
    "    >>> dataframes_dict = {'df1': pd.DataFrame({'Group': ['A', 'A', 'B', 'B'], 'Values1': [10, 15, 5, 8]}),\n",
    "                           'df2': pd.DataFrame({'Group': ['C', 'C', 'D', 'D'], 'Values1': [50, 55, 45, 60]})}\n",
    "    >>> min_max_normalize_multiple_inplace(dataframes_dict, group_column='Group', target_columns=['Values1'])\n",
    "    >>> print(dataframes_dict['df1'])\n",
    "       Group  Values1  Normalized_Values1\n",
    "    0     A       10                0.00\n",
    "    1     A       15                1.00\n",
    "    2     B        5                0.00\n",
    "    3     B        8                1.00\n",
    "    \"\"\"\n",
    "    # If new_column_prefix is not provided, create a default name\n",
    "    if new_column_prefix is None:\n",
    "        new_column_prefix = 'Normalized_'\n",
    "\n",
    "    # Iterate through DataFrames in the dictionary\n",
    "    for df_name, df in dataframes_dict.items():\n",
    "        # Iterate through target columns in each DataFrame\n",
    "        for target_column in target_columns:\n",
    "            # Calculate the min and max values for each group in the current DataFrame\n",
    "            min_values = df.groupby(group_column)[target_column].transform('min')\n",
    "            max_values = df.groupby(group_column)[target_column].transform('max')\n",
    "\n",
    "            # Add a new column with the normalized values to the existing DataFrame\n",
    "            new_column_name = f\"{new_column_prefix}{target_column}\"\n",
    "            df[new_column_name] = (df[target_column] - min_values) / (max_values - min_values)\n",
    "\n",
    "        # Update the dictionary with the modified DataFrame\n",
    "        dataframes_dict[df_name] = df\n",
    "\n",
    "        \n",
    "def load_gg_csv_files_to_dict(folder_path, prefix=\"prefix_\"):\n",
    "    \"\"\"\n",
    "    Load CSV files from a folder and create a dictionary of DataFrames with prefixed keys.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): The path to the folder containing CSV files.\n",
    "    - prefix (str): The prefix to be added to the keys of the dictionary (default is \"prefix_\").\n",
    "\n",
    "    Returns:\n",
    "    - dataframes_dict (dict): A dictionary where keys are prefixed numbers, and values are corresponding DataFrames.\n",
    "    \"\"\"\n",
    "    dataframes_dict = {}  # Dictionary to store DataFrames\n",
    "\n",
    "    # Iterate through files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file is a CSV file\n",
    "        if filename.endswith(\".csv\"):\n",
    "            # Extract the number from the filename\n",
    "            file_number = filename.split(\"_\")[0]\n",
    "\n",
    "            # Read the CSV file into a DataFrame\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            dataframe = pd.read_csv(file_path)\n",
    "\n",
    "            # Add the prefix to the number and use it as the key in the dictionary\n",
    "            key = f\"{prefix}{file_number}\"\n",
    "            dataframes_dict[key] = dataframe\n",
    "\n",
    "    return dataframes_dict\n",
    "\n",
    "\n",
    "def remove_spaces_from_column_headings_inplace(dataframes_dict):\n",
    "    \"\"\"\n",
    "    Remove spaces at the start and end of column headings in pandas DataFrames stored in a dictionary.\n",
    "    Replace the old columns with the modified column headings in-place.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes_dict (dict): A dictionary where keys are DataFrame names and values are DataFrames.\n",
    "\n",
    "    Modifies the input DataFrames in-place by replacing old columns with modified column headings.\n",
    "    \"\"\"\n",
    "    # Iterate through DataFrames in the dictionary\n",
    "    for df_name, df in dataframes_dict.items():\n",
    "        # Remove spaces at the start and end of column headings and replace the old columns\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        # Update the dictionary with the modified DataFrame\n",
    "        dataframes_dict[df_name] = df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1f405b29f92ee6ab",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb8ec275",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:23.001405Z",
     "start_time": "2023-12-13T10:02:22.719840Z"
    }
   },
   "source": [
    "# modify the below paths to point to wherever the gelgenie (gg)/gelanalyzer (ga)/ reference data is stored\n",
    "gg_path = Path(\"/Users/matt/Documents/PhD/research_output/Automatic_Gel_Analyzer/quantitative_results/qupath_data/james_data_v3_fixed_global/Data_with_rolling_ball\")\n",
    "ga_path = Path(\"/Users/matt/Documents/PhD/research_output/Automatic_Gel_Analyzer/quantitative_results/gelanalyzer\")\n",
    "reference_path = Path(\"/Users/matt/Documents/PhD/research_output/Automatic_Gel_Analyzer/quantitative_results/reference_ladder_masses.csv\")\n",
    "\n",
    "gg_dfs = load_gg_csv_files_to_dict(gg_path, \"gg_\") # loads data and converts to dictionary\n",
    "remove_spaces_from_column_headings_inplace(gg_dfs) # deletes spaces from column headings (due to an old bug that has now been fixed)\n",
    "\n",
    "gg_dfs = {key: gg_dfs[key] for key in sorted(gg_dfs.keys(), key=lambda s: [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', s)])}  # sorts by gel ID\n",
    "\n",
    "ga_dfs = load_ga_csv_files_from_folders(ga_path)\n",
    "ga_dfs = {key: ga_dfs[key] for key in sorted(ga_dfs.keys(), key=lambda s: [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', s)])} # sorts by gel ID\n",
    "\n",
    "for key, df in ga_dfs.items():\n",
    "    ga_dfs[key] = df[df['Reliable Band'] != 0].copy() # removes all band data that has been deemed as unreliable\n",
    "\n",
    "reference_df = pd.read_csv(reference_path)\n",
    "reference_df.rename(columns={\"NEB ladder\": \"NEB\", \" ThermoFisher ladder\": \"Thermo\"}, inplace=True)\n",
    "reference_df[\"Band ID\"] = range(1, len(reference_df) + 1)\n",
    "reference_df = pd.melt(reference_df, id_vars=[\"Band ID\"], value_vars=[\"NEB\", \"Thermo\"], var_name=\"Ladder\", value_name=\"Intensity\")\n",
    "reference_df['Normalized_Intensity'] = reference_df.groupby(\"Ladder\")[\"Intensity\"].transform(lambda x: (x - x.min()) / (x.max() - x.min()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab4d4474e3c732c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:24.321544Z",
     "start_time": "2023-12-13T10:02:24.177424Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "print('Example gelgenie data:')\n",
    "gg_dfs['gg_0']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78f92f38226463c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:25.706967Z",
     "start_time": "2023-12-13T10:02:25.670360Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "print('Example gelanalyzer data:')\n",
    "ga_dfs['ga_31']"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d9de7cebd08f0b78",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8035e45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:28.207713Z",
     "start_time": "2023-12-13T10:02:27.941277Z"
    }
   },
   "source": [
    "ga_columns = ['Raw Volume', 'Background Corrected Volume']\n",
    "gg_columns = ['Raw Volume','Local Corrected Volume', 'Global Corrected Volume', 'Rolling Ball Corrected Volume']\n",
    "\n",
    "min_max_normalize_multiple_inplace(ga_dfs, 'Lane ID', ga_columns)  \n",
    "min_max_normalize_multiple_inplace(gg_dfs, 'Lane ID', gg_columns)\n",
    "\n",
    "normalized_gg_columns = ['Normalized_Raw Volume', 'Normalized_Local Corrected Volume', 'Normalized_Global Corrected Volume', 'Normalized_Rolling Ball Corrected Volume']\n",
    "normalized_ga_columns = ['Normalized_Raw Volume', 'Normalized_Background Corrected Volume']"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f93b670246530ca8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Concatenation and final DF generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ac9d5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:29.443988Z",
     "start_time": "2023-12-13T10:02:29.250495Z"
    }
   },
   "source": [
    "for key, df in gg_dfs.items():\n",
    "    df['App'] = \"GG\"\n",
    "    df['Gel ID'] = key\n",
    "    \n",
    "for key, df in ga_dfs.items():\n",
    "    df['App'] = \"GA\"\n",
    "    df['Gel ID'] = key\n",
    "    \n",
    "concatenated_df = pd.concat([df for df in gg_dfs.values()] + [df for df in ga_dfs.values()], ignore_index=True)\n",
    "concatenated_df['Gel ID'] = concatenated_df['Gel ID'].str.extract(r'_(\\d+)')\n",
    "concatenated_df['Gel ID'] = concatenated_df['Gel ID'].astype(int)\n",
    "concatenated_df['Ladder'] = \"Temp\"\n",
    "concatenated_df.loc[concatenated_df[\"Gel ID\"].isin([0,1,2,3,4,5,6,7,8,9,10,11,12,32,34]), \"Ladder\"] = \"Thermo\"\n",
    "concatenated_df.loc[concatenated_df[\"Gel ID\"].isin([13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,29,30,31,33]), \"Ladder\"] = \"NEB\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9efe880e1e99063",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:31.037885Z",
     "start_time": "2023-12-13T10:02:30.856511Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "concatenated_df[concatenated_df['Gel ID'] == 32]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7f8ef29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:34.648958Z",
     "start_time": "2023-12-13T10:02:34.034334Z"
    }
   },
   "source": [
    "sns.violinplot(data=concatenated_df, x='Lane ID', y='Normalized_Raw Volume', hue='App', split=True, inner='quart')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3a8b80c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:36.944381Z",
     "start_time": "2023-12-13T10:02:36.833046Z"
    }
   },
   "source": [
    "melted_df = pd.melt(concatenated_df, id_vars=['Lane ID', 'Band ID', 'Gel ID', 'App'],\n",
    "                    value_vars=['Pixel Count', 'Average Intensity', 'Raw Volume', 'Local Corrected Volume',\n",
    "                                'Global Corrected Volume', 'Normalized_Raw Volume',\n",
    "                                'Normalized_Local Corrected Volume', 'Normalized_Global Corrected Volume',\n",
    "                                'Background Corrected Volume', 'Normalized_Background Corrected Volume'],\n",
    "                    var_name='Variable', value_name='Volume')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d9cc9e3b1a862e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:02:38.776456Z",
     "start_time": "2023-12-13T10:02:38.704806Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "melted_df # this df contains all the data split into rows for every single variable e.g. there is a single row for a normalised background value of a single band ID and lane ID.  The descriptor is held in the Variable column and the value is held in the Volumn column"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2d9d346095c3031a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Analysis of error vs reference ladder values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e846d09a0063a8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Merging in reference values into main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7714e136",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:03:30.349986Z",
     "start_time": "2023-12-13T10:03:30.308612Z"
    }
   },
   "source": [
    "merged_df = pd.merge(concatenated_df, reference_df, on=['Band ID', 'Ladder'], how='left')\n",
    "merged_df['Expected Value'] = merged_df['Normalized_Intensity']\n",
    "merged_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f26d6181",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:03:30.376746Z",
     "start_time": "2023-12-13T10:03:30.335940Z"
    }
   },
   "source": [
    "# simple error used here\n",
    "merged_df[\"Raw Difference\"] = merged_df[\"Normalized_Raw Volume\"] - merged_df['Expected Value']\n",
    "merged_df[\"Local Difference\"] = merged_df[\"Normalized_Local Corrected Volume\"] - merged_df['Expected Value']\n",
    "merged_df[\"Global Difference\"] = merged_df[\"Normalized_Global Corrected Volume\"] - merged_df['Expected Value']\n",
    "merged_df[\"Background Difference\"] = merged_df[\"Normalized_Background Corrected Volume\"] - merged_df['Expected Value']\n",
    "merged_df[\"Rolling Ball Difference\"] = merged_df[\"Normalized_Rolling Ball Corrected Volume\"] - merged_df['Expected Value']\n",
    "merged_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f914a71f8c44939",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:03:30.396076Z",
     "start_time": "2023-12-13T10:03:30.372653Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "output_dir = '/Users/matt/Desktop/pdf_error_plots'\n",
    "create_dir_if_empty(output_dir)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4021eaee2d006242",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:19:51.252948Z",
     "start_time": "2023-12-13T10:19:50.956095Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "merged_df.to_csv(join(output_dir,'merged_df.csv'),index=False)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
