experiment_name = "MA_unet_internal_testing"  # configuration name
base_dir = '/Users/matt/Documents/PhD/research_output/Automatic_Gel_Analyzer/segmentation_models'

[processing]
base_hardware = "MA_mac"   # Where the program is run [EDDIE/PC]
device = "GPU"    # Which processor is used [GPU/CPU]
pe = 1          # How many parallel environments (cores) needed
memory = 64     # Required memory per core in GBytes

[data]
n_channels = 1 # channels to output to model
batch_size = 1  # Batch size for dataloader
num_workers = 1 # parallel threads to use to speed up data processing
dir_train_mask = '/Users/matt/Documents/PhD/research_output/Automatic_Gel_Analyzer/data/processed_gels/dummy_set/masks_train'
dir_train_img = '/Users/matt/Documents/PhD/research_output/Automatic_Gel_Analyzer/data/processed_gels/dummy_set/images_train'
dir_val_img = '/Users/matt/Documents/PhD/research_output/Automatic_Gel_Analyzer/data/processed_gels/dummy_set/images_val'
dir_val_mask = '/Users/matt/Documents/PhD/research_output/Automatic_Gel_Analyzer/data/processed_gels/dummy_set/masks_val'
split_training_dataset = false
apply_augmentations = true
padding = true # pads all image with zeros to same size to allow for a batch size higher than 1

[model]
model_name = 'smp_unet'   # model architecture name
classes = 2     # Number of possible segmentation classes
encoder_name = 'resnet18'
[training]
loss = 'both'
lr = 1e-5       # learning rate
epochs = 2     # Number of epochs to run
grad_scaler = false     # Use mixed precision for faster training
load_checkpoint = false    # Load model from a .pth file (Bool/epoch ID)
optimizer_type = 'adam' # Use adam optimizer
scheduler_type = 'ReduceLROnPlateau'  # No scheduler is used
save_checkpoint = true # Whether model checkpoints are saved
checkpoint_frequency = 1 # How often checkpoints are saved (per epoch)
model_cleanup_frequency = 20 # How often old checkpoints are deleted (in epochs)
wandb_track = false # Whether to track training with wandb
